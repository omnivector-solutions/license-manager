services:
  lm-api:
    build:
      context: ../lm-api
      dockerfile: Dockerfile
    container_name: lm-api
    environment:
      DATABASE_HOST: db-lm-api
      DATABASE_USER: compose-db-api-user
      DATABASE_PSWD: compose-db-api-pswd
      DATABASE_NAME: compose-db-api-name
      DATABASE_PORT: 5432
      ARMASEC_DOMAIN: keycloak.local:8080/realms/lm-local
      ARMASEC_USE_HTTPS: false
      ARMASEC_DEBUG: false
      LOG_LEVEL: DEBUG
    ports: ["7000:8000"]
    command: uvicorn lm_api.main:app --reload --workers 1 --host 0.0.0.0 --port 8000
    healthcheck:
      test: curl --fail http://localhost:8000/lm/health || exit 1
      interval: 5s
      retries: 10
      timeout: 5s
    depends_on:
      migration:
        condition: service_completed_successfully
      keycloak.local:
        condition: service_healthy
    networks:
      - lm-net
    volumes:
      - ../lm-api/lm_api:/app/lm_api

  migration:
    build:
      context: ../lm-api
      dockerfile: Dockerfile-ci
    container_name: migration
    environment:
      DATABASE_HOST: db-lm-api
      DATABASE_USER: compose-db-api-user
      DATABASE_PSWD: compose-db-api-pswd
      DATABASE_NAME: compose-db-api-name
      ARMASEC_DOMAIN: keycloak.local:8080/realms/lm-local
      ARMASEC_USE_HTTPS: false
      LOG_LEVEL: DEBUG
    command: uv run python -m alembic -c alembic/alembic.ini upgrade head
    depends_on:
      db-lm-api:
        condition: service_healthy
    networks:
      - lm-net

  db-lm-api:
    image: postgres:13-alpine
    container_name: db-lm-api
    environment:
      POSTGRES_USER: compose-db-api-user
      POSTGRES_PASSWORD: compose-db-api-pswd
      POSTGRES_DB: compose-db-api-name
    ports: ["5434:5432"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U compose-db-api-user -d compose-db-api-name"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lm-net

  lm-agent-all:
    build:
      dockerfile: ./lm-agent/Dockerfile
      context: ..
    container_name: lm-agent-all
    depends_on:
      slurmctld:
        condition: service_started
    volumes:
      - ./etc/lm-config/populate-lm-api.py:/app/populate-lm-api.py
      - ./etc/lm-config/populate-lm-simulator-api.py:/app/populate-lm-simulator-api.py
      - ./etc/lm-config/lm-agent-entrypoint.sh:/app/lm-agent-entrypoint.sh:ro
      - slurm_binaries:/opt/slurm:ro
      - slurm_config:/etc/slurm:ro
    entrypoint: ["/bin/bash", "/app/lm-agent-entrypoint.sh"]
    environment:
      PATH: /app/.venv/bin:/opt/slurm/view/bin:/opt/slurm/view/sbin:/usr/local/bin:/usr/bin:/bin
      LD_LIBRARY_PATH: /opt/slurm/view/lib/private:/opt/slurm/view/lib:/opt/slurm/view/lib/slurm
      DATABASE_TYPE: sqlite
      ENABLE_LM_SIMULATOR: "true"
      ENABLE_PROLOG_EPILOG_API: "true"
      LM_AGENT_BACKEND_BASE_URL: http://lm-api:8000
      LM_AGENT_LOG_BASE_DIR: /var/log/license-manager-agent
      LM_AGENT_CACHE_DIR: /var/cache/license-manager-agent
      LM_AGENT_LOG_LEVEL: DEBUG
      LM_AGENT_OIDC_DOMAIN: keycloak.local:8080/realms/lm-local
      LM_AGENT_OIDC_CLIENT_ID: local-slurm
      LM_AGENT_OIDC_CLIENT_SECRET: PNy6e5CflHCsQVRcBx3rDpyUdam9n8nv
      LM_AGENT_OIDC_USE_HTTPS: false
      LM_AGENT_SCONTROL_PATH: /opt/slurm/view/bin/scontrol
      LM_AGENT_SACCTMGR_PATH: /opt/slurm/view/bin/sacctmgr
      LM_AGENT_SQUEUE_PATH: /opt/slurm/view/bin/squeue
      LM_AGENT_LMUTIL_PATH: /app/lm-agent/.venv/bin/lmutil
      LM_AGENT_RLMUTIL_PATH: /app/lm-agent/.venv/bin/rlmutil
      LM_AGENT_LSDYNA_PATH: /app/lm-agent/.venv/bin/lstc_qrun
      LM_AGENT_LMXENDUTIL_PATH: /app/lm-agent/.venv/bin/lmxendutil
      LM_AGENT_OLIXTOOL_PATH: /app/lm-agent/.venv/bin/olixtool
      LM_AGENT_DSLICSRV_PATH: /app/lm-agent/.venv/bin/DSLicSrv
      LM_AGENT_LM_USER: root
      LM_AGENT_STAT_INTERVAL: 60
    expose: ["8081", "8080"]
    healthcheck:
      test: curl --fail http://localhost:8080/lm-sim/health || exit 1
      interval: 5s
      retries: 10
      timeout: 5s
    networks:
      - lm-net

  keycloak.local:
    image: keycloak/keycloak:18.0.0
    container_name: keycloak
    environment:
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=admin
      - KC_HEALTH_ENABLED=true
    ports: ["8081:8080"]
    command: ["start-dev", "--import-realm"]
    restart: always
    healthcheck:
      test: curl --fail http://localhost:8080/health/ready || exit 1
      interval: 5s
      retries: 10
      timeout: 5s
    networks:
      - lm-net
    volumes:
      - kc-realm-files:/opt/keycloak/data/import/
      - ./etc/lm-local.json:/opt/keycloak/data/import/lm-local.json

  mysql:
    image: mysql:8.0
    container_name: mysql
    hostname: mysql
    environment:
      - MYSQL_RANDOM_ROOT_PASSWORD=yes
      - MYSQL_DATABASE=slurm_acct_db
      - MYSQL_USER=slurm
      - MYSQL_PASSWORD=password
    networks:
      - lm-net
    volumes:
      - var_lib_mysql:/var/lib/mysql

  slurmctld:
    build:
      context: .
      dockerfile: Dockerfile-slurm
      args:
        - JWT_SECRET=${JWT_SECRET:-supersecret}
    image: slurm-docker-cluster
    container_name: slurmctld
    hostname: slurmctld
    expose: ["6817"]
    command: ["slurmctld"]
    depends_on:
      lm-api:
        condition: service_healthy
    networks:
      - lm-net
    volumes:
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
      - slurm_binaries:/opt/slurm
      - slurm_config:/etc/slurm
      - ./etc/lm-config/configure-prolog-epilog.py:/app/configure-prolog-epilog.py
      - ./etc/slurm-config/slurmctld_prolog.sh:/app/slurmctld_prolog.sh
      - ./etc/slurm-config/slurmctld_epilog.sh:/app/slurmctld_epilog.sh
      - ./etc/job_example.py:/nfs/job_example.py
      - ./etc/lm-config/seed-license-in-slurm.py:/app/seed-license-in-slurm.py

  slurmdbd:
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    container_name: slurmdbd
    hostname: slurmdbd
    expose: ["6819"]
    command: ["slurmdbd"]
    volumes:
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
      - slurm_config:/etc/slurm
    depends_on:
      - slurmctld
      - mysql
    networks:
      - lm-net

  c1:
    privileged: true
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    container_name: c1
    hostname: c1
    expose: ["6818"]
    command: ["slurmd"]
    depends_on:
      - slurmctld
    networks:
      - lm-net
    volumes:
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
      - slurm_config:/etc/slurm

  c2:
    privileged: true
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    container_name: c2
    hostname: c2
    expose: ["6818"]
    command: ["slurmd"]
    depends_on:
      - slurmctld
    networks:
      - lm-net
    volumes:
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
      - slurm_config:/etc/slurm

  slurmrestd:
    build:
      context: .
      dockerfile: Dockerfile-slurm
    image: slurm-docker-cluster
    container_name: slurmrestd
    hostname: slurmrestd
    user: "slurmrestd:slurmrestd"
    environment:
      - SLURMRESTD_SECURITY=disable_unshare_files,disable_unshare_sysv
      - SLURM_JWT=daemon
    ports: ["6820:6820"]
    expose: ["6820"]
    command: ["slurmrestd"]
    depends_on:
      - slurmctld
    networks:
      - lm-net
    volumes:
      - var_log_slurm:/var/log/slurm
      - ./slurm-fake-nfs:/nfs
      - ./slurm-work-dir:/slurm-work-dir
      - slurm_config:/etc/slurm

volumes:
  postgres_data:
  kc-realm-files:
  var_lib_mysql:
  var_log_slurm:
  slurm_binaries:
  slurm_config:

networks:
  lm-net:
    driver: bridge
